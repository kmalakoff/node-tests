'use strict';

var _interopRequireDefault = require("@babel/runtime/helpers/interopRequireDefault");

var _toConsumableArray2 = _interopRequireDefault(require("@babel/runtime/helpers/toConsumableArray"));

var common = require('../common');

var fixtures = require('../common/fixtures');

var assert = require('assert');

var fs = require('fs'); // Test that concurrent file read streams don’t interfere with each other’s
// contents, and that the chunks generated by the reads only retain a
// 'reasonable' amount of memory.
// Refs: https://github.com/nodejs/node/issues/21967


var filename = fixtures.path('loop.js'); // Some small non-homogeneous file.

var content = fs.readFileSync(filename);
var N = 2000;
var started = 0;
var done = 0;
var arrayBuffers = new Set();

function startRead() {
  ++started;
  var chunks = [];
  fs.createReadStream(filename).on('data', function (chunk) {
    chunks.push(chunk);
    arrayBuffers.add(chunk.buffer);
  }).on('end', common.mustCall(function () {
    if (started < N) startRead();
    assert.deepStrictEqual(Buffer.concat(chunks), content);

    if (++done === N) {
      var retainedMemory = (0, _toConsumableArray2["default"])(arrayBuffers).map(function (ab) {
        return ab.byteLength;
      }).reduce(function (a, b) {
        return a + b;
      });
      assert(retainedMemory / (N * content.length) <= 3, "Retaining ".concat(retainedMemory, " bytes in ABs for ").concat(N, " ") + "chunks of size ".concat(content.length));
    }
  }));
} // Don’t start the reads all at once – that way we would have to allocate
// a large amount of memory upfront.


for (var i = 0; i < 6; ++i) {
  startRead();
}